{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_JoeJohnson.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzFFcRoyMjAd"
      },
      "source": [
        "# **CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDORjUkS81t"
      },
      "source": [
        "## **Initial CNN (from tutorial)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Y4poPwEqt3"
      },
      "source": [
        "This section is from the provided tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFKFl3WBWyU"
      },
      "source": [
        "Set Number of Epochs to Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forkmB_gBXwB"
      },
      "source": [
        "numepochs = 1"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tXjl_dIAOoE"
      },
      "source": [
        "Importing torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxZZ-ATe_4Ja"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from prettytable import PrettyTable  \n",
        "\n",
        "# for an easy format later\n",
        "t = PrettyTable(['Trial', 'Accuracy', 'Time/Epoch', 'Initial Loss/Batch', 'Average Loss/Batch', 'Final Loss/Batch'])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W27xOz_sTEwV"
      },
      "source": [
        "Add GPU Support"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpoUITatTISg",
        "outputId": "5daba570-64b1-41c1-8289-980566509ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ep5z-M5Agp9"
      },
      "source": [
        "Transforming imagers to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdRvjmlpASey",
        "outputId": "80bd8f54-d294-4100-cb5d-7002e36e9b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yTLx2GBncD"
      },
      "source": [
        "Modified for 3-channel images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GslyQ8AdAlvI",
        "outputId": "855debcb-893c-442b-f388-3ff32e144f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 1000, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(1000, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "#speed\n",
        "net.to(device)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 1000, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(1000, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qB534Q9CDDy"
      },
      "source": [
        "Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqe2KHKpCJVR"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz-dSjPBCMeO"
      },
      "source": [
        "train network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKyf58i8CWZu",
        "outputId": "3c7fdfd7-128c-4dd0-ef47-37f2acdbab78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tic = time.perf_counter()\n",
        "numphases = 0\n",
        "initloss = 0.0\n",
        "aveloss = 0.0\n",
        "for epoch in range(numepochs):  # loop over the dataset multiple times\n",
        "    \n",
        "    \n",
        "    finloss = 0.0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        " \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            numphases += 1\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            if initloss == 0:\n",
        "                initloss = running_loss / 2000\n",
        "            aveloss += running_loss / 2000\n",
        "            finloss = running_loss / 2000\n",
        "            running_loss = 0.0\n",
        "    \n",
        "aveloss = aveloss / numphases\n",
        "toc = time.perf_counter()\n",
        "runtime = (toc - tic)/numepochs\n",
        "print('Finished Training\\n')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 1.947\n",
            "[1,  4000] loss: 1.629\n",
            "[1,  6000] loss: 1.510\n",
            "[1,  8000] loss: 1.454\n",
            "[1, 10000] loss: 1.367\n",
            "[1, 12000] loss: 1.312\n",
            "Finished Training\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBibWpEXEmMt"
      },
      "source": [
        "use network on whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEQdM_TPFPGP",
        "outputId": "3d517e33-4c65-48d8-bc67-5014cdb07b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "acc = (100 * correct / total)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % acc)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 50 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCA6I8PXFXgj",
        "outputId": "6a3611bd-77bc-46cb-c36e-51619b5fa37c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 27 %\n",
            "Accuracy of   car : 86 %\n",
            "Accuracy of  bird : 21 %\n",
            "Accuracy of   cat : 19 %\n",
            "Accuracy of  deer : 42 %\n",
            "Accuracy of   dog : 52 %\n",
            "Accuracy of  frog : 64 %\n",
            "Accuracy of horse : 74 %\n",
            "Accuracy of  ship : 62 %\n",
            "Accuracy of truck : 52 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7K0gxU_GsT-"
      },
      "source": [
        "Summary Table addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moHzBFDiGrHX"
      },
      "source": [
        "# add to summary table\n",
        "t.add_row(['Base', '%2d %%' % acc, \n",
        "        '{:.0f}m {:.0f}s'.format(runtime // 60, runtime % 60), \n",
        "        '%.3f' % initloss, '%.3f' % aveloss, '%.3f' % finloss])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBN-HNqfRCoS"
      },
      "source": [
        "## **DNN experimentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43yYIRCKjQkO"
      },
      "source": [
        "I had problems with not re-initializing things, so for safety I will re-init everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzjyWUM72Sco"
      },
      "source": [
        "### **Learning Rate Schedule**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeA7k21kl9as"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "var_lr_list = ['.00001', '.0001', '.01', '.1']"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srMS-Wz6lGYk",
        "outputId": "ce5a27a9-f8bb-456d-c1a4-ad4edc16b92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for var_lr in var_lr_list:\n",
        "    print('\\nRunning with %s lr:\\n' % var_lr)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 1000, 5)\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            self.conv2 = nn.Conv2d(1000, 16, 5)\n",
        "            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = x.view(-1, 16 * 5 * 5)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "    net = Net()\n",
        "\n",
        "    #speed\n",
        "    net.to(device)\n",
        "\n",
        "    import torch.optim as optim\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr = float(var_lr), momentum = 0.9)\n",
        "\n",
        "    initloss = 0.0\n",
        "    phasecount = 0\n",
        "    aveloss = 0.0\n",
        "    \n",
        "    for epoch in range(numepochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        \n",
        "        finloss = 0.0\n",
        "        tic = time.perf_counter()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                phasecount += 1\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                if initloss == 0:\n",
        "                    initloss = running_loss / 2000\n",
        "                aveloss += running_loss / 2000\n",
        "                finloss = running_loss / 2000\n",
        "                running_loss = 0.0\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    \n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "    acc = (100 * correct / total)\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % acc)\n",
        "\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(4):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "        print('Accuracy of %5s : %2d %%' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "    toc = time.perf_counter()\n",
        "    runtime = (toc - tic)\n",
        "    aveloss = aveloss / phasecount\n",
        "    t.add_row(['L Rate = %s' % var_lr, '%2d %%' % acc, \n",
        "            '{:.0f}m {:.0f}s'.format(runtime // 60, runtime % 60), \n",
        "            '%.3f' % initloss, '%.3f' % (aveloss / phasecount), '%.3f' % finloss])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running with .00001 lr:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.301\n",
            "[1,  4000] loss: 2.298\n",
            "[1,  6000] loss: 2.293\n",
            "[1,  8000] loss: 2.287\n",
            "[1, 10000] loss: 2.277\n",
            "[1, 12000] loss: 2.262\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 13 %\n",
            "Accuracy of plane :  0 %\n",
            "Accuracy of   car :  6 %\n",
            "Accuracy of  bird :  1 %\n",
            "Accuracy of   cat :  1 %\n",
            "Accuracy of  deer :  0 %\n",
            "Accuracy of   dog :  0 %\n",
            "Accuracy of  frog : 15 %\n",
            "Accuracy of horse : 89 %\n",
            "Accuracy of  ship : 20 %\n",
            "Accuracy of truck :  1 %\n",
            "\n",
            "Running with .0001 lr:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.249\n",
            "[1,  4000] loss: 1.995\n",
            "[1,  6000] loss: 1.852\n",
            "[1,  8000] loss: 1.733\n",
            "[1, 10000] loss: 1.651\n",
            "[1, 12000] loss: 1.586\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 43 %\n",
            "Accuracy of plane : 43 %\n",
            "Accuracy of   car : 70 %\n",
            "Accuracy of  bird : 25 %\n",
            "Accuracy of   cat : 43 %\n",
            "Accuracy of  deer : 13 %\n",
            "Accuracy of   dog : 32 %\n",
            "Accuracy of  frog : 72 %\n",
            "Accuracy of horse : 50 %\n",
            "Accuracy of  ship : 48 %\n",
            "Accuracy of truck : 32 %\n",
            "\n",
            "Running with .01 lr:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.164\n",
            "[1,  4000] loss: 2.121\n",
            "[1,  6000] loss: 2.134\n",
            "[1,  8000] loss: 2.126\n",
            "[1, 10000] loss: 2.127\n",
            "[1, 12000] loss: 2.100\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "Accuracy of plane : 62 %\n",
            "Accuracy of   car :  6 %\n",
            "Accuracy of  bird :  0 %\n",
            "Accuracy of   cat :  0 %\n",
            "Accuracy of  deer :  0 %\n",
            "Accuracy of   dog :  1 %\n",
            "Accuracy of  frog : 97 %\n",
            "Accuracy of horse :  0 %\n",
            "Accuracy of  ship :  0 %\n",
            "Accuracy of truck :  0 %\n",
            "\n",
            "Running with .1 lr:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.360\n",
            "[1,  4000] loss: 2.361\n",
            "[1,  6000] loss: 2.362\n",
            "[1,  8000] loss: 2.363\n",
            "[1, 10000] loss: 2.360\n",
            "[1, 12000] loss: 2.359\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "Accuracy of plane : 100 %\n",
            "Accuracy of   car :  0 %\n",
            "Accuracy of  bird :  0 %\n",
            "Accuracy of   cat :  0 %\n",
            "Accuracy of  deer :  0 %\n",
            "Accuracy of   dog :  0 %\n",
            "Accuracy of  frog :  0 %\n",
            "Accuracy of horse :  0 %\n",
            "Accuracy of  ship :  0 %\n",
            "Accuracy of truck :  0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dijPtNSisZzk"
      },
      "source": [
        "### **Momentum Shift**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1abqxUIYsZzn"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "var_mom_list = ['.3', '.5', '.95', '.99']"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R04hl5wsZzq",
        "outputId": "fe32c5f5-d1ff-4d88-abac-fde2677f5c5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for var_mom in var_mom_list:\n",
        "    print('\\nRunning with %s momentum:\\n' % var_mom)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 1000, 5)\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            self.conv2 = nn.Conv2d(1000, 16, 5)\n",
        "            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = x.view(-1, 16 * 5 * 5)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "    net = Net()\n",
        "\n",
        "    #speed\n",
        "    net.to(device)\n",
        "\n",
        "    import torch.optim as optim\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr = .001, momentum = float(var_mom))\n",
        "    \n",
        "    initloss = 0.0\n",
        "    numphases = 0\n",
        "    aveloss = 0.0\n",
        "    \n",
        "    for epoch in range(numepochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        \n",
        "        finloss = 0.0\n",
        "        tic = time.perf_counter()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                numphases += 1\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                if initloss == 0:\n",
        "                    initloss = running_loss / 2000\n",
        "                aveloss += running_loss / 2000\n",
        "                finloss = running_loss / 2000\n",
        "                running_loss = 0.0\n",
        "        \n",
        "\n",
        "    print('Finished Training')\n",
        "    \n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "    acc = (100 * correct / total)\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % acc)\n",
        "\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(4):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "        print('Accuracy of %5s : %2d %%' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "    toc = time.perf_counter()\n",
        "    runtime = (toc - tic)\n",
        "    t.add_row(['Momentum = %s' % var_mom, '%2d %%' % acc, \n",
        "            '{:.0f}m {:.0f}s'.format(runtime // 60, runtime % 60), \n",
        "            '%.3f' % initloss, '%.3f' % (aveloss / numphases), '%.3f' % finloss])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running with .3 momentum:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.184\n",
            "[1,  4000] loss: 1.870\n",
            "[1,  6000] loss: 1.712\n",
            "[1,  8000] loss: 1.617\n",
            "[1, 10000] loss: 1.557\n",
            "[1, 12000] loss: 1.517\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 46 %\n",
            "Accuracy of plane : 52 %\n",
            "Accuracy of   car : 54 %\n",
            "Accuracy of  bird : 15 %\n",
            "Accuracy of   cat : 23 %\n",
            "Accuracy of  deer : 29 %\n",
            "Accuracy of   dog : 59 %\n",
            "Accuracy of  frog : 73 %\n",
            "Accuracy of horse : 59 %\n",
            "Accuracy of  ship : 58 %\n",
            "Accuracy of truck : 37 %\n",
            "\n",
            "Running with .5 momentum:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.142\n",
            "[1,  4000] loss: 1.867\n",
            "[1,  6000] loss: 1.694\n",
            "[1,  8000] loss: 1.592\n",
            "[1, 10000] loss: 1.519\n",
            "[1, 12000] loss: 1.479\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 49 %\n",
            "Accuracy of plane : 54 %\n",
            "Accuracy of   car : 75 %\n",
            "Accuracy of  bird : 33 %\n",
            "Accuracy of   cat : 28 %\n",
            "Accuracy of  deer : 38 %\n",
            "Accuracy of   dog : 38 %\n",
            "Accuracy of  frog : 64 %\n",
            "Accuracy of horse : 58 %\n",
            "Accuracy of  ship : 63 %\n",
            "Accuracy of truck : 38 %\n",
            "\n",
            "Running with .95 momentum:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 1.921\n",
            "[1,  4000] loss: 1.686\n",
            "[1,  6000] loss: 1.583\n",
            "[1,  8000] loss: 1.540\n",
            "[1, 10000] loss: 1.475\n",
            "[1, 12000] loss: 1.449\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 47 %\n",
            "Accuracy of plane : 52 %\n",
            "Accuracy of   car : 83 %\n",
            "Accuracy of  bird : 35 %\n",
            "Accuracy of   cat : 48 %\n",
            "Accuracy of  deer : 31 %\n",
            "Accuracy of   dog : 27 %\n",
            "Accuracy of  frog : 69 %\n",
            "Accuracy of horse : 44 %\n",
            "Accuracy of  ship : 39 %\n",
            "Accuracy of truck : 41 %\n",
            "\n",
            "Running with .99 momentum:\n",
            "\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.203\n",
            "[1,  4000] loss: 2.131\n",
            "[1,  6000] loss: 2.143\n",
            "[1,  8000] loss: 2.295\n",
            "[1, 10000] loss: 2.309\n",
            "[1, 12000] loss: 2.310\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "Accuracy of plane :  0 %\n",
            "Accuracy of   car :  0 %\n",
            "Accuracy of  bird :  0 %\n",
            "Accuracy of   cat : 100 %\n",
            "Accuracy of  deer :  0 %\n",
            "Accuracy of   dog :  0 %\n",
            "Accuracy of  frog :  0 %\n",
            "Accuracy of horse :  0 %\n",
            "Accuracy of  ship :  0 %\n",
            "Accuracy of truck :  0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAa27CZhrsmk"
      },
      "source": [
        "### **Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUkx5jiEAKI"
      },
      "source": [
        "Prints a summary for easy analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhFWItl7r3yn",
        "outputId": "090fea55-9a60-41dd-ff35-86cc981c5181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Base settings: Simple CNN, lr = .001, momentum = .9')\n",
        "print('Epochs/Trial: %2d\\n' % numepochs)\n",
        "print(t)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base settings: Simple CNN, lr = .001, momentum = .9\n",
            "Epochs/Trial:  1\n",
            "\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n",
            "|      Trial      | Accuracy | Time/Epoch | Initial Loss/Batch | Average Loss/Batch | Final Loss/Batch |\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n",
            "|       Base      |   50 %   |   1m 3s    |       1.947        |       1.536        |      1.312       |\n",
            "| L Rate = .00001 |   13 %   |   1m 22s   |       2.301        |       2.287        |      2.262       |\n",
            "|  L Rate = .0001 |   43 %   |   1m 23s   |       2.249        |       1.844        |      1.586       |\n",
            "|   L Rate = .01  |   16 %   |   1m 22s   |       2.164        |       2.129        |      2.100       |\n",
            "|   L Rate = .1   |   10 %   |   1m 22s   |       2.360        |       2.361        |      2.359       |\n",
            "|  Momentum = .3  |   46 %   |   1m 22s   |       2.184        |       1.743        |      1.517       |\n",
            "|  Momentum = .5  |   49 %   |   1m 22s   |       2.142        |       1.715        |      1.479       |\n",
            "|  Momentum = .95 |   47 %   |   1m 22s   |       1.921        |       1.609        |      1.449       |\n",
            "|  Momentum = .99 |   10 %   |   1m 22s   |       2.203        |       2.232        |      2.310       |\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O1MYHsqQv-D"
      },
      "source": [
        "## **Different Architectures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9hsECC-bYaj"
      },
      "source": [
        "Using examples from: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBv7QFGfbjI0"
      },
      "source": [
        "Going to re-import things here so I can run this section stand alone if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIBBlPJrbqce"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfMXePsjc5RZ"
      },
      "source": [
        "Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYegUsKLc-aw"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 10\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Batch size for training \n",
        "batch_size = 4\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = numepochs\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "# Classes\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZYlXx86cGsS"
      },
      "source": [
        "**Helper Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJ2jdj0cbiZ"
      },
      "source": [
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AnyIzqcIBv"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=num_epochs, is_inception=False):\n",
        "    tic = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    numphases = 0\n",
        "    initloss = 0.0\n",
        "    aveloss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        finloss = 0.0    \n",
        "        \n",
        "        \n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            run_phase = 0.0\n",
        "            i = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                run_phase += loss.item()\n",
        "\n",
        "                # statistics\n",
        "                if (i % 2000 == 1999) and (phase == 'train'):\n",
        "                    numphases += 1\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, run_phase / 2000))\n",
        "                    if initloss == 0:\n",
        "                        initloss = run_phase / 2000\n",
        "                    finloss = run_phase / 2000\n",
        "                    run_phase = 0.0\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                i += 1\n",
        "                \n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            \n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            aveloss += epoch_loss\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "            print()\n",
        "            \n",
        "\n",
        "    runtime = time.time() - tic\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(runtime // 60, runtime % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    \n",
        "    \n",
        "    t.add_row(['%s' % model_name, '%3d %%' % (100 * epoch_acc), \n",
        "        '{:.0f}m {:.0f}s'.format((runtime / num_epochs) // 60, (runtime / num_epochs) % 60), \n",
        "        '%.3f' % initloss, '%.3f' % (aveloss / num_epochs), '%.3f' % finloss])\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O1Optxicc9I"
      },
      "source": [
        "Helper to set .requires_grad to false by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hay_umVtcn5e"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81nLCzthSoe"
      },
      "source": [
        "Initialize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3NoX1HphXcA"
      },
      "source": [
        "choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
        "\n",
        "\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us1t2Zida07Q"
      },
      "source": [
        "### **Run for all classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84pwWT4Bdzhy",
        "outputId": "caf291e9-603c-4155-b89d-9a14dbe87dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "choices = [\"resnet\", \"alexnet\", \"vgg\", \"squeezenet\", \"densenet\", \"inception\"]\n",
        "\n",
        "for model_name in choices:\n",
        "    print('\\nRunning %s:\\n' % model_name)\n",
        "\n",
        "    # Initialize the model for this run\n",
        "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(input_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(input_size),\n",
        "            transforms.CenterCrop(input_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "    # Create training and validation datasets\n",
        "    image_datasets = {\n",
        "        'train': torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=data_transforms['train']),\n",
        "        'val': torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=data_transforms['val'])\n",
        "    }\n",
        "    # Create training and validation dataloaders\n",
        "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "    # Detect if we have a GPU available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Send the model to GPU\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    # Gather the parameters to be optimized/updated in this run. If we are\n",
        "    #  finetuning we will be updating all parameters. However, if we are\n",
        "    #  doing feature extract method, we will only update the parameters\n",
        "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
        "    #  is True.\n",
        "    params_to_update = model_ft.parameters()\n",
        "    print(\"Params to learn:\")\n",
        "    if feature_extract:\n",
        "        params_to_update = []\n",
        "        for name,param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\",name)\n",
        "    else:\n",
        "        for name,param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                print(\"\\t\",name)\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Setup the loss fxn\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train and evaluate\n",
        "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running resnet:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 1.893\n",
            "[1,  4000] loss: 1.730\n",
            "[1,  6000] loss: 1.722\n",
            "[1,  8000] loss: 1.719\n",
            "[1, 10000] loss: 1.726\n",
            "[1, 12000] loss: 1.699\n",
            "train Loss: 1.7478 Acc: 0.4240\n",
            "\n",
            "val Loss: 0.8180 Acc: 0.7250\n",
            "\n",
            "Training complete in 1m 53s\n",
            "Best val Acc: 0.725000\n",
            "\n",
            "Running alexnet:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 2.646\n",
            "[1,  4000] loss: 2.813\n",
            "[1,  6000] loss: 2.753\n",
            "[1,  8000] loss: 2.740\n",
            "[1, 10000] loss: 2.784\n",
            "[1, 12000] loss: 2.951\n",
            "train Loss: 2.7802 Acc: 0.4181\n",
            "\n",
            "val Loss: 1.4133 Acc: 0.6722\n",
            "\n",
            "Training complete in 1m 12s\n",
            "Best val Acc: 0.672200\n",
            "\n",
            "Running vgg:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 1.704\n",
            "[1,  4000] loss: 1.702\n",
            "[1,  6000] loss: 1.753\n",
            "[1,  8000] loss: 1.756\n",
            "[1, 10000] loss: 1.770\n",
            "[1, 12000] loss: 1.812\n",
            "train Loss: 1.7533 Acc: 0.4408\n",
            "\n",
            "val Loss: 0.9384 Acc: 0.6896\n",
            "\n",
            "Training complete in 3m 2s\n",
            "Best val Acc: 0.689600\n",
            "\n",
            "Running squeezenet:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 1.707\n",
            "[1,  4000] loss: 1.499\n",
            "[1,  6000] loss: 1.448\n",
            "[1,  8000] loss: 1.430\n",
            "[1, 10000] loss: 1.398\n",
            "[1, 12000] loss: 1.369\n",
            "train Loss: 1.4711 Acc: 0.4830\n",
            "\n",
            "val Loss: 0.9978 Acc: 0.6757\n",
            "\n",
            "Training complete in 1m 44s\n",
            "Best val Acc: 0.675700\n",
            "\n",
            "Running densenet:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t classifier.weight\n",
            "\t classifier.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 1.873\n",
            "[1,  4000] loss: 1.762\n",
            "[1,  6000] loss: 1.697\n",
            "[1,  8000] loss: 1.715\n",
            "[1, 10000] loss: 1.689\n",
            "[1, 12000] loss: 1.701\n",
            "train Loss: 1.7379 Acc: 0.4234\n",
            "\n",
            "val Loss: 0.9304 Acc: 0.7014\n",
            "\n",
            "Training complete in 6m 7s\n",
            "Best val Acc: 0.701400\n",
            "\n",
            "Running inception:\n",
            "\n",
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Params to learn:\n",
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n",
            "Epoch 1/1\n",
            "----------\n",
            "[1,  2000] loss: 2.738\n",
            "[1,  4000] loss: 2.536\n",
            "[1,  6000] loss: 2.554\n",
            "[1,  8000] loss: 2.553\n",
            "[1, 10000] loss: 2.552\n",
            "[1, 12000] loss: 2.566\n",
            "train Loss: 2.5810 Acc: 0.3429\n",
            "\n",
            "val Loss: 1.1962 Acc: 0.5926\n",
            "\n",
            "Training complete in 6m 48s\n",
            "Best val Acc: 0.592600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0jTE7lnWqY"
      },
      "source": [
        "### **Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_wYGMoInWqZ"
      },
      "source": [
        "Prints a summary for easy analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1yMbannWqZ",
        "outputId": "ee0dfa2a-b645-4efc-8296-b84f3ec2708b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Base settings: Simple CNN, lr = .001, momentum = .9')\n",
        "print('Epochs/Trial: %2d\\n' % numepochs)\n",
        "print(t)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base settings: Simple CNN, lr = .001, momentum = .9\n",
            "Epochs/Trial:  1\n",
            "\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n",
            "|      Trial      | Accuracy | Time/Epoch | Initial Loss/Batch | Average Loss/Batch | Final Loss/Batch |\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n",
            "|       Base      |   50 %   |   1m 3s    |       1.947        |       1.536        |      1.312       |\n",
            "| L Rate = .00001 |   13 %   |   1m 22s   |       2.301        |       2.287        |      2.262       |\n",
            "|  L Rate = .0001 |   43 %   |   1m 23s   |       2.249        |       1.844        |      1.586       |\n",
            "|   L Rate = .01  |   16 %   |   1m 22s   |       2.164        |       2.129        |      2.100       |\n",
            "|   L Rate = .1   |   10 %   |   1m 22s   |       2.360        |       2.361        |      2.359       |\n",
            "|  Momentum = .3  |   46 %   |   1m 22s   |       2.184        |       1.743        |      1.517       |\n",
            "|  Momentum = .5  |   49 %   |   1m 22s   |       2.142        |       1.715        |      1.479       |\n",
            "|  Momentum = .95 |   47 %   |   1m 22s   |       1.921        |       1.609        |      1.449       |\n",
            "|  Momentum = .99 |   10 %   |   1m 22s   |       2.203        |       2.232        |      2.310       |\n",
            "|      resnet     |   72 %   |   1m 53s   |       1.893        |       2.566        |      1.699       |\n",
            "|     alexnet     |   67 %   |   1m 12s   |       2.646        |       4.194        |      2.951       |\n",
            "|       vgg       |   68 %   |   3m 2s    |       1.704        |       2.692        |      1.812       |\n",
            "|    squeezenet   |   67 %   |   1m 44s   |       1.707        |       2.469        |      1.369       |\n",
            "|     densenet    |   70 %   |   6m 7s    |       1.873        |       2.668        |      1.701       |\n",
            "|    inception    |   59 %   |   6m 48s   |       2.738        |       3.777        |      2.566       |\n",
            "+-----------------+----------+------------+--------------------+--------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc3h6ywkMsXB"
      },
      "source": [
        "# **Linear Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9HkBg1UAmR"
      },
      "source": [
        "Got a start from http://cs231n.stanford.edu/2017/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwLPdgb6_U6k"
      },
      "source": [
        "import torch, torchvision, PIL\n",
        "from torchvision.datasets import CIFAR10 \n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import torchvision.transforms as transforms\n",
        "import IPython.display\n",
        "\n",
        "imgTransform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "class2id = {name: idx for (idx, name) in enumerate(classes)}\n",
        "\n",
        "trainset = CIFAR10(root='./data', train = True, transform = imgTransform)\n",
        "valset = CIFAR10(root='./data', train = False, transform = imgTransform)"
      ],
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTtYlhA_gD7",
        "outputId": "f0837b95-ece1-4b8c-fb9c-198ecd8267f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Softmax function: exp(a) / sum(exp(a))\n",
        "# Note that this function works for tensors of any shape, it is not a scalar function.\n",
        "def softmax(a):\n",
        "    max_val = a.max()  # This is to avoid variable overflows.\n",
        "    exp_a = (a - max_val).exp()\n",
        "    return exp_a.div(exp_a.sum())\n",
        "\n",
        "# Classification function: y = softmax(Wx + b)\n",
        "def linear(x, weight, bias):\n",
        "    return torch.matmul(weight, x) + bias\n",
        "\n",
        "# Initialize bias and weight with random values.\n",
        "weight = torch.Tensor(10, 3 * 32 * 32).normal_(0, 0.01)\n",
        "bias = torch.Tensor(10, 1).normal_(0, 0.01)\n",
        "\n",
        "# Now predict the category using this un-trained classifier.\n",
        "img, _ = trainset[0]\n",
        "x = img.view(3 * 32 * 32, 1)\n",
        "a = linear(x, weight, bias)\n",
        "predictions = softmax(a)\n",
        "\n",
        "# Show the results of the classifier.\n",
        "max_score, max_label = predictions.max(0)\n",
        "print('Image predicted as %s with confidence %.2f' % (classes[max_label[0]], max_score[0]))"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image predicted as ship with confidence 0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8xbi4JC_g5p",
        "outputId": "64fa6c99-1f34-45fb-df9b-f6af2b1685c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss(label, predictions):\n",
        "    return -predictions[label].log()"
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_hat[airplane] = 0.06\n",
            "y_hat[automobile] = 0.11\n",
            "y_hat[bird] = 0.11\n",
            "y_hat[cat] = 0.08\n",
            "y_hat[deer] = 0.10\n",
            "y_hat[dog] = 0.09\n",
            "y_hat[frog] = 0.08\n",
            "y_hat[horse] = 0.12\n",
            "y_hat[ship] = 0.13\n",
            "y_hat[truck] = 0.12\n",
            "\n",
            "Loss: 2.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KcEH1uB_k72"
      },
      "source": [
        "epsilon = 0.0001\n",
        "shifted_weight = weight.clone()  # Make a copy of the weights.\n",
        "\n",
        "# Initialize gradients for bias and weight with zero values.\n",
        "gradWeight = torch.Tensor(10, 3 * 32 * 32).fill_(0)\n",
        "gradBias = torch.Tensor(10, 1).fill_(0)\n",
        "\n",
        "# Compute gradients for each weight w_ij\n",
        "for i in range(0, weight.shape[0]):\n",
        "    for j in range(0, weight.shape[1]):\n",
        "        # Compute f(x + h)\n",
        "        shifted_weight[i, j] = shifted_weight[i, j] + epsilon\n",
        "        f1 = softmax(linear(x, shifted_weight, bias))\n",
        "        loss1 = loss(class2id['frog'], f1)\n",
        "        shifted_weight[i, j] = weight[i, j] # restore original value.\n",
        "        \n",
        "        # Compute f(x - h)\n",
        "        shifted_weight[i, j] = shifted_weight[i, j] - epsilon\n",
        "        f2 = softmax(linear(x, shifted_weight, bias))\n",
        "        loss2 = loss(class2id['frog'], f2)\n",
        "        shifted_weight[i, j] = weight[i, j] # restore original value.\n",
        "\n",
        "        # Compute [f(x + h) - f(x - h)] / 2h.\n",
        "        gradWeight[i, j] = (loss1[0] - loss2[0]) / (2 * epsilon)\n",
        "\n",
        "numericalGradWeight = gradWeight "
      ],
      "execution_count": 470,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B76brwkA_nC1",
        "outputId": "93a11d22-3509-4058-f31b-2db680a253ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss_softmax_backward(label, predictions):\n",
        "    grad_inputs = predictions.clone()\n",
        "    grad_inputs[label] = grad_inputs[label] - 1\n",
        "    return grad_inputs\n",
        "\n",
        "def linear_backward(x, weight, bias, gradOutput):\n",
        "    gradBias = bias.clone().zero_()\n",
        "    gradWeight = weight.clone().zero_()\n",
        "    gradWeight = gradOutput * x.t()\n",
        "    gradBias.copy_(gradOutput)\n",
        "    return gradWeight, gradBias\n",
        "\n",
        "gradOutput = loss_softmax_backward(class2id['frog'], predictions)\n",
        "gradWeight, gradBias = linear_backward(x, weight, bias, gradOutput)\n",
        "\n",
        "# Let's print the gradWeight again.\n",
        "print(gradWeight)\n",
        "\n",
        "g1 = gradWeight.view(-1, 1).squeeze()\n",
        "g2 = numericalGradWeight.view(-1, 1).squeeze()\n",
        "print('Distance betwen numerical and analytical gradients: %.6f' % \n",
        "      (torch.norm(g1 - g2) / torch.norm(g1 + g2)))"
      ],
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0139, 0.0102, 0.0118,  ..., 0.0331, 0.0199, 0.0170],\n",
            "        [0.0247, 0.0180, 0.0209,  ..., 0.0587, 0.0352, 0.0302],\n",
            "        [0.0258, 0.0188, 0.0219,  ..., 0.0613, 0.0368, 0.0315],\n",
            "        ...,\n",
            "        [0.0287, 0.0209, 0.0243,  ..., 0.0682, 0.0409, 0.0351],\n",
            "        [0.0294, 0.0214, 0.0249,  ..., 0.0698, 0.0419, 0.0359],\n",
            "        [0.0277, 0.0202, 0.0234,  ..., 0.0656, 0.0394, 0.0337]])\n",
            "Distance betwen numerical and analytical gradients: 0.001869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60PVCNi2_pGT",
        "outputId": "7dca191a-a009-4c35-fd8a-e8942c172f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "learningRates = [1e-4, 1e-5]\n",
        "weightDecay = 1e-6 # Regularization strength.\n",
        "\n",
        "# Initialize bias and weight with random values again.\n",
        "weight = torch.Tensor(10, 3 * 32 * 32).normal_(0, 0.1)\n",
        "bias = torch.Tensor(10, 1).normal_(0, 10)\n",
        "print('Testing with biases:\\n%s\\n' % bias)\n",
        "\n",
        "tic = time.perf_counter()\n",
        "for epoch in range(0, numepochs):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    learningRate = learningRates[0] if epoch < 5 else learningRates[1]\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (img, label)) in enumerate(trainset):\n",
        "        x = img.view(3 * 32 * 32, 1)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        predictions = softmax(linear(x, weight, bias))\n",
        "        cum_loss += loss(label, predictions)[0]\n",
        "        max_score, max_label = predictions.max(0)\n",
        "        if max_label[0] == label: correct += 1\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        gradOutput = loss_softmax_backward(label, predictions)\n",
        "        gradWeight, gradBias = linear_backward(x, weight, bias, gradOutput)\n",
        "        \n",
        "        # Parameter updates.\n",
        "        gradWeight.add_(weightDecay, weight)\n",
        "        weight.add_(-learningRate, gradWeight)\n",
        "        #bias = bias - learningRate * gradBias\n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i + 1)))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (img, label)) in enumerate(valset):\n",
        "        x = img.view(3 * 32 * 32, 1)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        predictions = softmax(linear(x, weight, bias))\n",
        "        cum_loss += loss(label, predictions)[0]\n",
        "        max_score, max_label = predictions.max(0)\n",
        "        if max_label[0] == label: correct += 1\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / len(valset), correct / len(valset)))\n",
        "    \n",
        "toc = time.perf_counter()\n",
        "print('%5d seconds' % ((toc - tic)/num_epochs))\n",
        "\n",
        "# Initialize weight with, bias to 0\n",
        "weight = torch.Tensor(10, 3 * 32 * 32).normal_(0, 0.01)\n",
        "bias = torch.Tensor(10, 1).zero_()\n",
        "\n",
        "\n",
        "print('Testing with biases:\\n%s\\n' % bias)\n",
        "tic = time.perf_counter()\n",
        "for epoch in range(0, numepochs):\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    learningRate = learningRates[0] if epoch < 5 else learningRates[1]\n",
        "    \n",
        "    # Make a pass over the training data.\n",
        "    for (i, (img, label)) in enumerate(trainset):\n",
        "        x = img.view(3 * 32 * 32, 1)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        predictions = softmax(linear(x, weight, bias))\n",
        "        cum_loss += loss(label, predictions)[0]\n",
        "        max_score, max_label = predictions.max(0)\n",
        "        if max_label[0] == label: correct += 1\n",
        "        \n",
        "        #Backward pass. (Gradient computation stage)\n",
        "        gradOutput = loss_softmax_backward(label, predictions)\n",
        "        gradWeight, gradBias = linear_backward(x, weight, bias, gradOutput)\n",
        "        \n",
        "        # Parameter updates.\n",
        "        gradWeight.add_(weightDecay, weight)\n",
        "        weight.add_(-learningRate, gradWeight)\n",
        "        #bias = bias - learningRate * gradBias\n",
        "        \n",
        "        # Logging the current results on training.\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "                  (epoch, i + 1, cum_loss / (i + 1), correct / (i + 1)))\n",
        "    \n",
        "    \n",
        "    # Make a pass over the validation data.\n",
        "    correct = 0.0\n",
        "    cum_loss = 0.0\n",
        "    for (i, (img, label)) in enumerate(valset):\n",
        "        x = img.view(3 * 32 * 32, 1)\n",
        "        \n",
        "        # Forward pass. (Prediction stage)\n",
        "        predictions = softmax(linear(x, weight, bias))\n",
        "        cum_loss += loss(label, predictions)[0]\n",
        "        max_score, max_label = predictions.max(0)\n",
        "        if max_label[0] == label: correct += 1\n",
        "            \n",
        "    # Logging the current results on validation.\n",
        "    print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
        "          (epoch, cum_loss / len(valset), correct / len(valset)))\n",
        "    \n",
        "print('Confirming biases:\\n%s\\n' % bias)\n",
        "toc = time.perf_counter()\n",
        "print('%5d seconds' % ((toc - tic)/num_epochs))\n",
        "        "
      ],
      "execution_count": 472,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing with biases:\n",
            "tensor([[ 2.1986],\n",
            "        [-1.1776],\n",
            "        [11.7782],\n",
            "        [ 3.4704],\n",
            "        [-5.4980],\n",
            "        [ 5.2877],\n",
            "        [ 7.3121],\n",
            "        [ 1.5822],\n",
            "        [-1.6860],\n",
            "        [ 9.9831]])\n",
            "\n",
            "Train-epoch 0. Iteration 10000, Avg-Loss: 3.1818, Accuracy: 0.1583\n",
            "Train-epoch 0. Iteration 20000, Avg-Loss: 2.9427, Accuracy: 0.1776\n",
            "Train-epoch 0. Iteration 30000, Avg-Loss: 2.8110, Accuracy: 0.1958\n",
            "Train-epoch 0. Iteration 40000, Avg-Loss: 2.7309, Accuracy: 0.2069\n",
            "Train-epoch 0. Iteration 50000, Avg-Loss: 2.6747, Accuracy: 0.2151\n",
            "Validation-epoch 0. Avg-Loss: 2.4220, Accuracy: 0.2559\n",
            "Train-epoch 1. Iteration 10000, Avg-Loss: 2.4052, Accuracy: 0.2593\n",
            "Train-epoch 1. Iteration 20000, Avg-Loss: 2.4094, Accuracy: 0.2587\n",
            "Train-epoch 1. Iteration 30000, Avg-Loss: 2.3901, Accuracy: 0.2650\n",
            "Train-epoch 1. Iteration 40000, Avg-Loss: 2.3806, Accuracy: 0.2687\n",
            "Train-epoch 1. Iteration 50000, Avg-Loss: 2.3732, Accuracy: 0.2699\n",
            "Validation-epoch 1. Avg-Loss: 2.3285, Accuracy: 0.2820\n",
            "Train-epoch 2. Iteration 10000, Avg-Loss: 2.3135, Accuracy: 0.2797\n",
            "Train-epoch 2. Iteration 20000, Avg-Loss: 2.3259, Accuracy: 0.2786\n",
            "Train-epoch 2. Iteration 30000, Avg-Loss: 2.3121, Accuracy: 0.2848\n",
            "Train-epoch 2. Iteration 40000, Avg-Loss: 2.3074, Accuracy: 0.2876\n",
            "Train-epoch 2. Iteration 50000, Avg-Loss: 2.3048, Accuracy: 0.2882\n",
            "Validation-epoch 2. Avg-Loss: 2.2835, Accuracy: 0.2954\n",
            "Train-epoch 3. Iteration 10000, Avg-Loss: 2.2659, Accuracy: 0.2957\n",
            "Train-epoch 3. Iteration 20000, Avg-Loss: 2.2810, Accuracy: 0.2933\n",
            "Train-epoch 3. Iteration 30000, Avg-Loss: 2.2691, Accuracy: 0.2974\n",
            "Train-epoch 3. Iteration 40000, Avg-Loss: 2.2662, Accuracy: 0.2999\n",
            "Train-epoch 3. Iteration 50000, Avg-Loss: 2.2655, Accuracy: 0.3000\n",
            "Validation-epoch 3. Avg-Loss: 2.2552, Accuracy: 0.3042\n",
            "Train-epoch 4. Iteration 10000, Avg-Loss: 2.2347, Accuracy: 0.3058\n",
            "Train-epoch 4. Iteration 20000, Avg-Loss: 2.2508, Accuracy: 0.3031\n",
            "Train-epoch 4. Iteration 30000, Avg-Loss: 2.2400, Accuracy: 0.3063\n",
            "Train-epoch 4. Iteration 40000, Avg-Loss: 2.2379, Accuracy: 0.3081\n",
            "Train-epoch 4. Iteration 50000, Avg-Loss: 2.2383, Accuracy: 0.3080\n",
            "Validation-epoch 4. Avg-Loss: 2.2351, Accuracy: 0.3099\n",
            "   26 seconds\n",
            "Testing with biases:\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "\n",
            "Train-epoch 0. Iteration 10000, Avg-Loss: 2.1262, Accuracy: 0.2331\n",
            "Train-epoch 0. Iteration 20000, Avg-Loss: 2.0623, Accuracy: 0.2633\n",
            "Train-epoch 0. Iteration 30000, Avg-Loss: 2.0150, Accuracy: 0.2880\n",
            "Train-epoch 0. Iteration 40000, Avg-Loss: 1.9880, Accuracy: 0.3008\n",
            "Train-epoch 0. Iteration 50000, Avg-Loss: 1.9691, Accuracy: 0.3081\n",
            "Validation-epoch 0. Avg-Loss: 1.8704, Accuracy: 0.3478\n",
            "Train-epoch 1. Iteration 10000, Avg-Loss: 1.8562, Accuracy: 0.3518\n",
            "Train-epoch 1. Iteration 20000, Avg-Loss: 1.8625, Accuracy: 0.3513\n",
            "Train-epoch 1. Iteration 30000, Avg-Loss: 1.8503, Accuracy: 0.3576\n",
            "Train-epoch 1. Iteration 40000, Avg-Loss: 1.8476, Accuracy: 0.3587\n",
            "Train-epoch 1. Iteration 50000, Avg-Loss: 1.8469, Accuracy: 0.3584\n",
            "Validation-epoch 1. Avg-Loss: 1.8239, Accuracy: 0.3662\n",
            "Train-epoch 2. Iteration 10000, Avg-Loss: 1.8098, Accuracy: 0.3724\n",
            "Train-epoch 2. Iteration 20000, Avg-Loss: 1.8217, Accuracy: 0.3690\n",
            "Train-epoch 2. Iteration 30000, Avg-Loss: 1.8120, Accuracy: 0.3735\n",
            "Train-epoch 2. Iteration 40000, Avg-Loss: 1.8117, Accuracy: 0.3740\n",
            "Train-epoch 2. Iteration 50000, Avg-Loss: 1.8134, Accuracy: 0.3730\n",
            "Validation-epoch 2. Avg-Loss: 1.8020, Accuracy: 0.3739\n",
            "Train-epoch 3. Iteration 10000, Avg-Loss: 1.7857, Accuracy: 0.3822\n",
            "Train-epoch 3. Iteration 20000, Avg-Loss: 1.7997, Accuracy: 0.3776\n",
            "Train-epoch 3. Iteration 30000, Avg-Loss: 1.7910, Accuracy: 0.3813\n",
            "Train-epoch 3. Iteration 40000, Avg-Loss: 1.7914, Accuracy: 0.3819\n",
            "Train-epoch 3. Iteration 50000, Avg-Loss: 1.7940, Accuracy: 0.3806\n",
            "Validation-epoch 3. Avg-Loss: 1.7884, Accuracy: 0.3801\n",
            "Train-epoch 4. Iteration 10000, Avg-Loss: 1.7698, Accuracy: 0.3883\n",
            "Train-epoch 4. Iteration 20000, Avg-Loss: 1.7850, Accuracy: 0.3832\n",
            "Train-epoch 4. Iteration 30000, Avg-Loss: 1.7767, Accuracy: 0.3865\n",
            "Train-epoch 4. Iteration 40000, Avg-Loss: 1.7775, Accuracy: 0.3873\n",
            "Train-epoch 4. Iteration 50000, Avg-Loss: 1.7805, Accuracy: 0.3856\n",
            "Validation-epoch 4. Avg-Loss: 1.7789, Accuracy: 0.3837\n",
            "Confirming biases:\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "\n",
            "   26 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWs1SOw4NC7t"
      },
      "source": [
        "# **Nearest Neighbor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnA2p5v951FZ"
      },
      "source": [
        "*(Never got this working)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2IlNDCg7J2W"
      },
      "source": [
        "**Uncomment to install pykeops for Lazy Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-vlTC3A4-Yz"
      },
      "source": [
        "#!pip install pykeops[full] > install.log"
      ],
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xongIodSl51A"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pykeops.torch import LazyTensor\n",
        "use_cuda = torch.cuda.is_available()\n",
        "tensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP83gMcyKOhM"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWokdQeAIVCA",
        "outputId": "fab5b85c-c0a4-4158-914c-52a21f9aa4fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                    download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8tweM5YkIma"
      },
      "source": [
        "x = tensor(trainset.data.astype('float32'))\n",
        "y = tensor(testset.data.astype('int64'))"
      ],
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG2GDanEv-zm"
      },
      "source": [
        "D = x.shape[1]\n",
        "Ntrain, Ntest = (60000, 10000) if use_cuda else (1000, 100)\n",
        "x_train, y_train = x[:Ntrain,:].contiguous(), y[:Ntrain].contiguous()\n",
        "x_test,  y_test  = x[Ntrain:Ntrain+Ntest,:].contiguous(), y[Ntrain:Ntrain+Ntest].contiguous()"
      ],
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW6qdr2qiiWb",
        "outputId": "9293eaf3-e990-4ecd-d8f7-f6282d474ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "K = 3  # N.B.: K has very little impact on the running time\n",
        "\n",
        "start = time.time()    # Benchmark:\n",
        "\n",
        "X_i = LazyTensor(x_test[:, None, :])  #  test set\n",
        "X_j = LazyTensor(x_train[None, :, :])  #  train set\n",
        "D_ij = ((X_i - X_j) ** 2).sum(-1)  # (10000, 60000) symbolic matrix of squared L2 distances\n",
        "\n",
        "ind_knn = D_ij.argKmin(K, dim=1)  # Samples <-> Dataset, (N_test, K)\n",
        "lab_knn = y_train[ind_knn]  # (N_test, K) array of integers in [0,9]\n",
        "y_knn, _ = lab_knn.mode()   # Compute the most likely label\n",
        "\n",
        "if use_cuda: torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "\n",
        "error = (y_knn != y_test).float().mean().item()\n",
        "time  = end - start\n",
        "\n",
        "print(\"{}-NN test error = {:.2f}% in {:.2f}s.\".format(K, error*100, time))"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-398-de86b495819a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Benchmark:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mD_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_j\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (10000, 60000) symbolic matrix of squared L2 distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pykeops/common/lazy_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, axis)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 226\u001b[0;31m                         \"If 'x' is a 3D+ tensor, its shape should be one of (..,M,1,D), (..,1,N,D) or (..,1,1,D).\")\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Stage 4: x is now encoded as a 2D or 1D array + batch dimensions --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: If 'x' is a 3D+ tensor, its shape should be one of (..,M,1,D), (..,1,N,D) or (..,1,1,D)."
          ]
        }
      ]
    }
  ]
}